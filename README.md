# udacity-data-lake-s3-spark

## Steps

- copy files using s3-dist-cp to hdfs, then load to spark from there
- prototype read and transform in jupyter
- write to hdfs, then use s3-dist-cp to copy to s3
- create script to launch jobs from local onto the cluster

Other todos:

- Create bucket in us-west-2